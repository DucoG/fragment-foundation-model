task_name: pretrain_fragformer
tags:
- dev
print_config: true
ignore_warnings: false
enforce_tags: false
train: true
test_after_training: false
test_from_ckpt: false
test_from_ckpt_path: null
seed: 42
datamodule:
  _target_: fragformer.datamodule.pretrain_datamodule.PretrainDataModule
  parquet_path: ${oc.env:PARQUET_PATH}
  columns:
  - read1_seq
  - read2_seq
  context_window: 25
  tokenizer:
    _target_: fragformer.datamodule.components.tokenizers.BasicTokenizer
    vocabulary: ACGT
    add_special_tokens: true
  transform: null
  batch_size: 2048
  num_workers: 0
  pin_memory: true
callbacks:
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${oc.env:CHECKPOINT_PATH}pretrain
    filename: pretrain_checkpoint_{epoch:02d}-{train_loss_epoch:.3f}
    monitor: train_loss
    verbose: false
    save_last: true
    save_top_k: 5
    mode: min
    auto_insert_metric_name: true
    save_weights_only: false
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: 1
    save_on_train_epoch_end: true
  lr_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar
  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: 1
model:
  _target_: fragformer.models.fragment_xtransformer.FragmentXTransformer
  num_tokens: 9
  attn_layers:
    _target_: x_transformers.x_transformers.Encoder
    dim: 128
    depth: 2
    heads: 2
    ff_glu: true
    attn_dropout: 0.1
    ff_dropout: 0.1
transforms:
  fit:
    _target_: fragformer.transforms.transforms.TransformFactory
    transforms:
    - _target_: fragformer.transforms.masking.Masking
      mask_prob: 0.15
      replace_prob: 1.0
      _partial_: true
lit_module:
  _target_: fragformer.lit_modules.lit_module_pretrain.FragFormerLitModule
  optimizer:
    _target_: torch.optim.AdamW
    lr: 0.005
    eps: 1.0e-06
    betas:
    - 0.9
    - 0.999
    weight_decay: 0.01
    _partial_: true
  scheduler:
    _target_: fragformer.utils.model_utils.CosineWarmupScheduler
    _partial_: true
    warmup: 180000
    max_iters: 9000000
  scheduler_interval: step
loss:
  _target_: fragformer.loss.LossFactory
  losses:
  - mlm_loss:
      _target_: fragformer.loss.MLMLoss
      weight: 1.0
      pad_token_id: 0
logger:
  wandb:
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    save_dir: ${paths.output_dir}
    offline: false
    id: null
    anonymous: null
    project: lightning-hydra-template
    log_model: true
    prefix: ''
    group: ''
    tags: []
    job_type: ''
trainer:
  _target_: lightning.pytorch.Trainer
  accelerator: gpu
  devices: 1
  max_epochs: 500
  log_every_n_steps: 1
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  log_dir: ${paths.root_dir}/logs
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}
