task_name: debug_fragformer
tags:
- dev
print_config: true
ignore_warnings: false
enforce_tags: false
train: true
test_after_training: false
test_from_ckpt: false
test_from_ckpt_path: null
seed: 42
datamodule:
  _target_: fragformer.datamodule.pretrain_datamodule.PretrainDataModule
  parquet_path: ${oc.env:PARQUET_PATH}
  columns:
  - read1_seq
  - read2_seq
  context_window: 25
  tokenizer:
    _target_: fragformer.datamodule.components.tokenizers.BasicTokenizer
    vocabulary: ACGT
    add_special_tokens: true
  transform: null
  batch_size: 2048
  num_workers: 0
  pin_memory: false
callbacks: null
model:
  _target_: fragformer.models.fragment_xtransformer.FragmentXTransformer
  num_tokens: 9
  attn_layers:
    _target_: x_transformers.x_transformers.Encoder
    dim: 128
    depth: 2
    heads: 2
    ff_glu: true
    attn_dropout: 0.1
    ff_dropout: 0.1
transforms:
  fit:
    _target_: fragformer.transforms.transforms.TransformFactory
    transforms:
    - _target_: fragformer.transforms.masking.Masking
      mask_prob: 0.15
      replace_prob: 1.0
      _partial_: true
lit_module:
  _target_: fragformer.lit_modules.lit_module_pretrain.FragFormerLitModule
  optimizer:
    _target_: torch.optim.AdamW
    lr: 0.005
    eps: 1.0e-06
    betas:
    - 0.9
    - 0.999
    weight_decay: 0.01
    _partial_: true
  scheduler:
    _target_: fragformer.utils.model_utils.CosineWarmupScheduler
    _partial_: true
    warmup: 10
    max_iters: 500
loss:
  _target_: fragformer.loss.LossFactory
  losses:
  - mlm_loss:
      _target_: fragformer.loss.MLMLoss
      weight: 1.0
      pad_token_id: 0
logger: null
trainer:
  _target_: lightning.pytorch.Trainer
  accelerator: cpu
  devices: 1
  max_epochs: 1
  log_every_n_steps: 1
  detect_anomaly: true
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  log_dir: ${paths.root_dir}/logs
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}
extras:
  ignore_warnings: false
  enforce_tags: false
