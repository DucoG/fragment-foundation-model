Start script:
current time Wed 24 Apr 2024 03:21:08 PM CEST
CONFIG
├── trainer
│   └── _target_: lightning.pytorch.Trainer                                     
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       max_epochs: 500                                                         
│       log_every_n_steps: 1                                                    
│                                                                               
├── model
│   └── _target_: fragformer.models.fragment_xtransformer.FragmentXTransformer  
│       num_tokens: 9                                                           
│       attn_layers:                                                            
│         _target_: x_transformers.x_transformers.Encoder                       
│         dim: 128                                                              
│         depth: 2                                                              
│         heads: 2                                                              
│         ff_glu: true                                                          
│         attn_dropout: 0.1                                                     
│         ff_dropout: 0.1                                                       
│                                                                               
├── experiment
│   └── None                                                                    
├── datamodule
│   └── _target_: fragformer.datamodule.pretrain_datamodule.PretrainDataModule  
│       parquet_path: /processing/d.gaillard/top1_mal_full_fragments/PGDX25828P_
│       columns:                                                                
│       - read1_seq                                                             
│       - read2_seq                                                             
│       context_window: 25                                                      
│       tokenizer:                                                              
│         _target_: fragformer.datamodule.components.tokenizers.BasicTokenizer  
│         vocabulary: ACGT                                                      
│         add_special_tokens: true                                              
│       transform: null                                                         
│       batch_size: 8192                                                        
│       num_workers: 0                                                          
│       pin_memory: true                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
│         dirpath: /home/d.gaillard/projects/fragment_autoencoder/fragment_found
│         filename: pretrain_checkpoint_{epoch:02d}-{train_loss_epoch:.3f}      
│         monitor: train_loss                                                   
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 5                                                         
│         mode: min                                                             
│         auto_insert_metric_name: true                                         
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: 1                                                     
│         save_on_train_epoch_end: true                                         
│       lr_monitor:                                                             
│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             
│       rich_progress_bar:                                                      
│         _target_: lightning.pytorch.callbacks.RichProgressBar                 
│       model_summary:                                                          
│         _target_: lightning.pytorch.callbacks.RichModelSummary                
│         max_depth: 1                                                          
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: lightning.pytorch.loggers.wandb.WandbLogger                 
│         save_dir: /home/d.gaillard/projects/fragment_autoencoder/fragment_foun
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: lightning-hydra-template                                     
│         log_model: true                                                       
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── test_after_training
│   └── False                                                                   
├── seed
│   └── 42                                                                      
└── name
    └── None                                                                    
[2024-04-24 15:21:13,678][lightning_fabric.utilities.seed][INFO] - [rank: 0] Seed set to 42
[2024-04-24 15:21:13,681][fragformer.entrypoints][INFO] - Instantiating datamodule <fragformer.datamodule.pretrain_datamodule.PretrainDataModule>
[2024-04-24 15:21:14,038][fragformer.entrypoints][INFO] - Instantiating fit augmentations <fragformer.transforms.transforms.TransformFactory>
[2024-04-24 15:21:14,043][fragformer.entrypoints][INFO] - Instantiating model <fragformer.models.fragment_xtransformer.FragmentXTransformer>
[2024-04-24 15:21:14,227][fragformer.entrypoints][INFO] - Instantiating loss <fragformer.loss.LossFactory>
[2024-04-24 15:21:14,230][fragformer.entrypoints][INFO] - Instantiating model <fragformer.lit_modules.lit_module_pretrain.FragFormerLitModule>
[2024-04-24 15:21:14,255][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpdildf_z7
[2024-04-24 15:21:14,255][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpdildf_z7/_remote_module_non_scriptable.py
[2024-04-24 15:21:14,285][fragformer.entrypoints][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-04-24 15:21:14,289][fragformer.entrypoints][INFO] - Instantiating callback <lightning.pytorch.callbacks.LearningRateMonitor>
[2024-04-24 15:21:14,289][fragformer.entrypoints][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-04-24 15:21:14,290][fragformer.entrypoints][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-04-24 15:21:14,290][fragformer.entrypoints][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-04-24 15:21:14,338][fragformer.entrypoints][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
[2024-04-24 15:21:15,660][lightning.pytorch.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-04-24 15:21:15,677][lightning.pytorch.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-04-24 15:21:15,770][lightning.pytorch.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-04-24 15:21:15,770][lightning.pytorch.utilities.rank_zero][INFO] - IPU available: False, using: 0 IPUs
[2024-04-24 15:21:15,770][lightning.pytorch.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-04-24 15:21:15,771][fragformer.entrypoints][INFO] - Logging hyperparameters...
[2024-04-24 15:21:29,891][fragformer.entrypoints][INFO] - Starting training...
[2024-04-24 15:21:29,947][lightning.pytorch.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type                 ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ model         │ FragmentXTransformer │  542 K │
│ 1 │ loss          │ LossFactory          │      0 │
│ 2 │ train_mlm_acc │ MulticlassAccuracy   │      0 │
│ 3 │ val_mlm_acc   │ MulticlassAccuracy   │      0 │
└───┴───────────────┴──────────────────────┴────────┘
Trainable params: 542 K                                                         
Non-trainable params: 0                                                         
Total params: 542 K                                                             
Total estimated model params size (MB): 2                                       
[2024-04-24 15:21:30,587][lightning.pytorch.trainer.connectors.signal_connector][INFO] - SLURM auto-requeueing enabled. Setting signal handlers.
Epoch 0/499 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/-- 0:00:00 • -:--:-- 0.00it/s 
current time Wed 24 Apr 2024 03:21:45 PM CEST
Finished
