/home/d.gaillard/source/miniconda3/envs/pytorch/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
INFO: Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
INFO: GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
wandb: Currently logged in as: ducogaillard. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home/d.gaillard/projects/fragment_autoencoder/fragment_foundation_model/logs/pretrain_fragformer/runs/2024-04-24_17-05-55/wandb/run-20240424_170603-r2nwr7y3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-serenity-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ducogaillard/lightning-hydra-template
wandb: üöÄ View run at https://wandb.ai/ducogaillard/lightning-hydra-template/runs/r2nwr7y3
/home/d.gaillard/source/miniconda3/envs/pytorch/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /home/d.gaillard/projects/fragment_autoencoder/fragment_foundation_model/tools/checkpoints/pretrain exists and is not empty.
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: SLURM auto-requeueing enabled. Setting signal handlers.
