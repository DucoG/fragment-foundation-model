/home/d.gaillard/source/miniconda3/envs/pytorch/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
INFO: Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
INFO: GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
wandb: Currently logged in as: ducogaillard. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home/d.gaillard/projects/fragment_autoencoder/fragment_foundation_model/logs/pretrain_fragformer/runs/2024-04-24_15-36-58/wandb/run-20240424_153704-u7yub5p1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-pond-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ducogaillard/lightning-hydra-template
wandb: üöÄ View run at https://wandb.ai/ducogaillard/lightning-hydra-template/runs/u7yub5p1
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: SLURM auto-requeueing enabled. Setting signal handlers.
